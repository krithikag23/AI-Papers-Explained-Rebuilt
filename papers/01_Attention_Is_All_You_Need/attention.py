"""
Scaled Dot-Product Attention (from 'Attention Is All You Need').

This implementation focuses on clarity and correctness.
It is meant for learning + reproducibility, not maximum speed.
"""


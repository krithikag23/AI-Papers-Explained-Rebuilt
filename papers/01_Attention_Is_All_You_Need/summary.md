# Attention Is All You Need — Summary

## Paper Overview
The paper *“Attention Is All You Need”* introduces the **Transformer architecture**, a novel neural network model that relies entirely on attention mechanisms, removing the need for recurrence (RNNs) and convolution (CNNs) in sequence modeling tasks.
This work fundamentally changed how natural language processing systems are built and laid the foundation for modern large language models.
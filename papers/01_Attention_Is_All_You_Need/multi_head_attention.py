"""
Multi-Head Attention (from 'Attention Is All You Need') implemented in NumPy.

This file builds on scaled dot-product attention and shows how multiple heads
can attend to different subspaces of the representation.
"""

